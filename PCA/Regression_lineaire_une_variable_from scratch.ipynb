{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression linéaire avec descente de gradient from scratch - une variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, on va réimplementer une régression linéaire **depuis zéro (from scratch)** en utilisant la méthode de **descente de gradient (gradient descent)**. On part du modèle suivant à une variable : $ y = f_{\\theta}(x) = \\theta_0 + \\theta_1 * x + \\epsilon$ \n",
    "\n",
    "où y désigne la variable cible (*target*) et x une variable explicative (*feature*).\n",
    "\n",
    "On cherche à estimer $\\theta_0$ et $\\theta_1$. Dans ce notebook, on va créer plusieurs fonctions intermédiaires pour implémenter la descente de gradient.On vérifiera ensuite les paramètres obtenus en sortie en utilisant des données simulées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Coder la fonction **predict_linear(x, theta0, theta1)** pour une régression linéaire utilisant une variable : $ \\hat{y} = f_{\\theta}(x) = \\theta_0 + \\theta_1 * x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear(x, theta0, theta1):\n",
    "    return theta0 + theta1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour trouver les paramètres d'un modèle, il est habituel d'utiliser la moyenne des erreurs au carré (Mean Squared Error - MSE) comme fonction de coût.\n",
    "\n",
    "2. En partant du modèle précédent et avoir défini la MSE,coder la fonction **mse(predictions, labels)** qui calcule la MSE entre les prédictions d'un modèle et les labels ($y$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predictions, labels):\n",
    "    sum_error = 0\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        prediction_error = (labels[i] - predictions[i])**2\n",
    "        sum_error+= (prediction_error)\n",
    "        \n",
    "    return float(sum_error / (2*len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Coder la fonction **grad(x**_**, y, theta0**_ **,theta1)** qui calcule le gradient de la fonction **de coût** pour la régression linéaire (la MSE). Le type de retour devra être un vecteur numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x, y, theta0,theta1):\n",
    "    cost_theta0 = 0.0\n",
    "    cost_theta1 = 0.0   \n",
    "    \n",
    "    for j in range(len(y)):\n",
    "        partial_deriv_theta0 = -2 * (y[j] - (theta0 + theta1*x[j]))\n",
    "        partial_deriv_theta1 = (-2 * x[j]) * (y[j] - (theta0 + theta1*x[j]))\n",
    "        \n",
    "        cost_theta0 +=partial_deriv_theta0\n",
    "        cost_theta1 += partial_deriv_theta1\n",
    "        \n",
    "    return float(cost_theta0),float(cost_theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Coder la fonction **compute_theta_sgd(theta0, theta1, learning_rate,x ,y)** et qui renvoie les nouveaux  $\\theta$ selon la Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta_sgd(theta0, theta1, learning_rate,x ,y):\n",
    "    \n",
    "    deriv0,deriv1 = grad(x, y, theta0,theta1)\n",
    "    \n",
    "    dividedby = 2\n",
    "    \n",
    "    new_theta0 = theta0 - (learning_rate * deriv0)\n",
    "    new_theta1 = theta1 - (learning_rate * deriv1) \n",
    "    \n",
    "    learning_rate = learning_rate/dividedby\n",
    "    \n",
    "    return float(new_theta0),float(new_theta1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Reprendre les fonctions précédentes et faire une fonction **gradient_descent(X, y, theta_0, theta_1, learning_rate, n_iterations)** pour effectuer la descente de gradient et qui retourne theta0 et theta1 après l'entraînement (sous forme de liste)\n",
    " - learning_rate : Taux d'apprentissage\n",
    " - n_iterations : Nombre d'itérations\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta0, theta1, learning_rate, n_iterations):\n",
    "    \n",
    "    theta0_L,theta1_L, MSE = [], [],[]\n",
    "    batch_size = X.shape[0] #lecture de toutes les obs - à modifier pour mini-batch ou online\n",
    "    #batch_size = 10\n",
    "    cur_iter = 0\n",
    "    \n",
    "    theta0_L.append(theta0)\n",
    "    theta1_L.append(theta1)\n",
    "  \n",
    "    theta_old = np.array([theta0, theta1])  \n",
    "    theta_new = np.array([0, 0]) \n",
    "\n",
    "    while(cur_iter<=n_iterations): #and (np.isclose(theta_new,theta_old, atol = 1e-8).any()==False):\n",
    "        indexes = np.random.randint(0, len(X), batch_size) # mini-batch avec random à chaque itération - eviter effets de cycle, mini-batch\n",
    "        Xs = np.take(X, indexes)\n",
    "        ys = np.take(y, indexes)\n",
    "        N = len(Xs)\n",
    "                                           \n",
    "        theta_old = np.array([theta0, theta1])  \n",
    "    \n",
    "        [theta0,theta1] = compute_theta_sgd(theta0, theta1, learning_rate,Xs ,ys)\n",
    "        \n",
    "        theta_new = np.array([theta0, theta1]) \n",
    "        theta0_L.append(theta0)\n",
    "        theta1_L.append(theta1)\n",
    "        MSE.append(mse(ys, theta0+theta1*Xs)) \n",
    "        \n",
    "        \n",
    "        cur_iter+=1\n",
    "        \n",
    "    return theta0_L, theta1_L,MSE, cur_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@nikhilparmar9/simple-sgd-implementation-in-python-for-linear-regression-on-boston-housing-data-f63fcaaecfb1\n",
    "https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f\n",
    "\n",
    "https://mrmint.fr/gradient-descent-algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Tester la fonction **gradient_descent(X, y, theta_0, theta_1, learning_rate, n_iterations)** sur les jeux de données suivants:\n",
    " - Jeu de données simulées (pouvant être modifié):\n",
    "     - theta0 = 5\n",
    "     - theta1 = 3\n",
    "     - X = 2 * np.random.rand(100,1)\n",
    "     - Y = theta0 +theta1 * X+np.random.randn(100,1)\n",
    "\n",
    "     - Initialiser theta0 et theta1 de façon aléatoire (via la fonction random) et comparer les derniers theta_0 et theta_1    générés dans la fonction **gradient_descent** aux valeurs choisies dans la simulation (theta0 = 2, theta1 = 3)\n",
    "\n",
    "\n",
    " - Jeu de données Boston Housing data\n",
    " \n",
    "    - Charger les données avec la commande `from sklearn.datasets import load_boston`\n",
    "    - Récupérer dans un dataframe les variables Y = `MEDV` (target - prix de l'immobilier) et X = `RM` (surface habitée)\n",
    "    - Faire la régression linéaire de Y en fonction de X puis récupérer les theta0 et theta1 estimés\n",
    "    - Lancer la fonction **gradient_descent** à partir de Y, X et en initialisant aléatoirement theta0 et theta1\n",
    "    - Comparer les theta obtenus en sortie à ceux de la régression linéaire via scikit-learn\n",
    "\n",
    "\n",
    " - Pour les 2 jeux de données:\n",
    " \n",
    "    - Tracer le nuage de points de Y en fonction de X\n",
    "    - Tracer la courbe de la fonction de coût en fonction des itérations\n",
    "    - Tracer les courbes de theta0 et theta1 en fonction des itérations\n",
    "    - Tracer des Y et des prédictions $ \\hat{y}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applications jeux de données simulées et Boston House pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1234)\n",
    "x = 2 * np.random.rand(100,1)\n",
    "y = 5 +3 * x+np.random.randn(100,1)\n",
    "theta0 = 0.4*np.random.rand(1,1)\n",
    "theta1 = 0.3*np.random.rand(1,1)\n",
    "nb_it = 1000\n",
    "l_r = 0.001\n",
    "#a learning rate taken within the set {.1, .01, 10−3, 10−4 , 10−5}\n",
    "\n",
    "\n",
    "theta0_L, theta1_L,MSE, cur_iter = gradient_descent(x, y, theta0, theta1, l_r, nb_it)\n",
    "print(f\"Les valeurs estimées sont {theta0_L[-1]} et {theta1_L[-1]} obtenus en {cur_iter} itérations\")\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.title('y = f(x)')\n",
    "plt.plot(x,predict_linear(x, theta0_L[-1], theta1_L[-1]))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(2,sharex=True)\n",
    "\n",
    "ax[0].plot(MSE)\n",
    "ax[0].set_title(\"MSE\")\n",
    "ax[1].plot(theta0_L)\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].plot(theta1_L)\n",
    "ax[1].set_ylabel(\"theta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "boston_data=pd.DataFrame(load_boston().data,columns=load_boston().feature_names)\n",
    "Y=load_boston().target\n",
    "X=boston_data['RM']\n",
    "print(Y.shape,X.shape)\n",
    "X = X.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "n_iter=1000\n",
    "clf_ = SGDRegressor(max_iter=n_iter)\n",
    "clf_.fit(X, Y)\n",
    "\n",
    "print(clf_.intercept_[0],clf_.coef_[0])\n",
    "\n",
    "theta0 = 0\n",
    "theta1 = 0\n",
    "\n",
    "theta0_L, theta1_L,MSE, cur_iter = gradient_descent(X, Y, theta0, theta1, 0.0001,1000)\n",
    "print(f\"Les valeurs estimées sont {theta0_L[-1]} et {theta1_L[-1]} obtenus en {cur_iter} itérations\")\n",
    "\n",
    "plt.scatter(X,Y)\n",
    "plt.title('y = f(x)')\n",
    "plt.plot(X,predict_linear(X, theta0_L[-1], theta1_L[-1]))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(2,sharex=True)\n",
    "\n",
    "ax[0].plot(MSE)\n",
    "ax[0].set_title(\"MSE\")\n",
    "ax[1].plot(theta0_L)\n",
    "ax[1].set_title(\"theta0 &theta1\")\n",
    "ax[1].plot(theta1_L)\n",
    "ax[1].set_ylabel(\"theta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
