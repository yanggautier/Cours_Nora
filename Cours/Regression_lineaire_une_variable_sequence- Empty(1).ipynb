{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression linéaire avec descente de gradient from scratch - une variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, on va réimplementer une régression linéaire **depuis zéro (from scratch)** en utilisant la méthode de **descente de gradient (gradient descent)**. On part du modèle suivant à une variable : $ y = f_{\\theta}(x) = \\theta_0 + \\theta_1 * x + \\epsilon$ \n",
    "\n",
    "où y désigne la variable cible (*target*) et x une variable explicative (*feature*).\n",
    "\n",
    "On cherche à estimer $\\theta_0$ et $\\theta_1$. Dans ce notebook, on va créer plusieurs fonctions intermédiaires pour implémenter la descente de gradient.On vérifiera ensuite les paramètres obtenus en sortie en utilisant des données simulées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Coder la fonction **predict_linear(x, theta0, theta1)** pour une régression linéaire utilisant une variable : $ \\hat{y} = f_{\\theta}(x) = \\theta_0 + \\theta_1 * x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:12:16.991255Z",
     "start_time": "2020-04-07T14:12:16.986241Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_linear(x, theta0, theta1):\n",
    "    return theta0 + theta1 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour trouver les paramètres d'un modèle, il est habituel d'utiliser la moyenne des erreurs au carré (Mean Squared Error - MSE) comme fonction de coût.\n",
    "\n",
    "2. En partant du modèle précédent et avoir défini la MSE,coder la fonction **mse(predictions, labels)** qui calcule la MSE entre les prédictions d'un modèle et les labels ($y$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T08:00:12.839959Z",
     "start_time": "2020-04-07T08:00:12.834973Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse(predictions, labels):\n",
    "    return sum((predictions[i] - labels[i])**2 for i in range(len(predictions)))/len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Coder la fonction **grad(x**_**, y, theta0**_ **,theta1)** qui calcule le gradient de la fonction **de coût** pour la régression linéaire (la MSE). Le type de retour devra être un vecteur numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T08:15:53.905967Z",
     "start_time": "2020-04-07T08:15:53.898959Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad(x, y, theta0, theta1):\n",
    "    dth1 = sum([x[i] * (theta1 * x[i] + theta0 - y[i]) for i in range(len(x))]) / len(x)\n",
    "    dth0 = sum([theta1 * x[i] + theta0 - y[i] for i in range(len(x))]) / len(x)\n",
    "    return np.array([dth1, dth0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Coder la fonction compute_theta_sgd(theta0, theta1, learning_rate,x ,y) et qui renvoie les nouveaux $/theta$ selon la Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T14:19:41.323188Z",
     "start_time": "2020-04-07T14:19:41.318173Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_theta_sgd(theta0, theta1, learning_rate, X, y):\n",
    "# to minimize the MSE, we have to go through : \n",
    "# a1 = a0 - lr.derivee_mse\n",
    "# if derivee_mse == 0 => a1 = a0\n",
    "# l'algo va trouver le minimum tout seul grace au changement de signe de la dérivée\n",
    "\n",
    "    coeff = theta0\n",
    "    intercept = theta1\n",
    "    new_coeff = 0\n",
    "    new_intercept = 0\n",
    "\n",
    "    while True:\n",
    "        derivee = grad(x, y, intercept, coeff)\n",
    "        new_coeff = coeff - learning_rate * derivee[0]\n",
    "        new_intercept = intercept - learning_rate * derivee[1]\n",
    "\n",
    "        # if True, cela veut dire que la dérivée est nulle, donc on peut trouver le minimum\n",
    "        if new_coeff == coeff:\n",
    "            break\n",
    "\n",
    "        coeff = new_coeff\n",
    "        intercept = new_intercept\n",
    "\n",
    "    return (new_coeff, new_intercept)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Reprendre les fonctions précédentes et faire une fonction **gradient_descent(X, y, theta_0, theta_1, learning_rate, n_iterations)** pour effectuer la descente de gradient et qui retourne theta0 et theta1 après l'entraînement (sous forme de liste)\n",
    " - learning_rate : Taux d'apprentissage\n",
    " - n_iterations : Nombre d'itérations\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  gradient_descent(X, y, theta_0, theta_1, learning_rate, n_iterations):\n",
    "    for i in \n",
    "    compute_theta_sgd(theta0, theta1, learning_rate, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_LR(X, Y, epochs, lr):\n",
    "    assert(len(X) == len(Y))\n",
    "    m = 0\n",
    "    b = 0\n",
    "    for e in range(epochs):\n",
    "        m = m - lr * m_grad(m, b, X, Y)\n",
    "        b = b - lr * b_grad(m, b, X, Y)\n",
    "    return m, b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
